import pandas as pd
import torch
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import re
import warnings
warnings.filterwarnings('ignore')

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# =====================================
# 1. ì „ì²˜ë¦¬ 
# =====================================
def clean_text(text):
    if pd.isna(text):
        return ""
    text = re.sub(r'\s+', ' ', str(text))
    text = text.strip()
    return text

# =====================================
# 2. KoELECTRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# =====================================
# KoELECTRA ëª¨ë¸ ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
model_paths = [
    './ai_detection_koelectra_model_0.95/best_koelectra_soft_cleaning',
    './ai_detection_koelectra_model_0.95',
    './ai_detection_koelectra_model_0.95/checkpoint-latest',
]

model_loaded = False
model_info = None

for model_path in model_paths:
    try:
        print(f"ëª¨ë¸ ë¡œë”© ì‹œë„ ì¤‘: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.to(device)
        model.eval()
        
        model_info = {
            'model': model,
            'tokenizer': tokenizer,
            'path': model_path,
            'name': 'koelectra-base-v3',
            'type': 'koelectra'
        }
        
        print(f"âœ… KoELECTRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ê²½ë¡œ: {model_path})")
        model_loaded = True
        break
        
    except Exception as e:
        print(f"âŒ {model_path} ë¡œë”© ì‹¤íŒ¨: {e}")
        continue

if not model_loaded:
    print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - KoELECTRA ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("í•™ìŠµëœ ëª¨ë¸ì´ ë‹¤ìŒ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:")
    for path in model_paths:
        print(f"  - {path}")
    exit()

print(f"\nâœ… KoELECTRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# =====================================
# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# =====================================
test_df = pd.read_csv('./test.csv', encoding='utf-8-sig')
test_df['paragraph_text'] = test_df['paragraph_text'].apply(clean_text)
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_df.shape}")

# =====================================
# 4. ë°°ì¹˜ ì˜ˆì¸¡ í•¨ìˆ˜
# =====================================
def predict_batch(texts, model, tokenizer, batch_size=32, max_length=512):
    all_probs = []
    
    for i in tqdm(range(0, len(texts), batch_size), desc="ë°°ì¹˜ ì˜ˆì¸¡"):
        batch_texts = texts[i:i+batch_size]
        
        # ë°°ì¹˜ í† í°í™”
        encoded = tokenizer(
            batch_texts,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors='pt'
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        encoded = {k: v.to(device) for k, v in encoded.items()}
        
        # ë°°ì¹˜ ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**encoded)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()
            all_probs.extend(probs)
    
    return np.array(all_probs)

# =====================================
# 5. KoELECTRA ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
# =====================================
print("\nKoELECTRA ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘...")

# ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰
predictions = predict_batch(
    test_df['paragraph_text'].tolist(),
    model_info['model'],
    model_info['tokenizer'],
    batch_size=16,  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
    max_length=512
)

print(f"âœ… KoELECTRA ì˜ˆì¸¡ ì™„ë£Œ - í‰ê·  ì˜ˆì¸¡ê°’: {predictions.mean():.4f}")

# =====================================
# 6. ê²°ê³¼ ì €ì¥
# =====================================
print("\nê²°ê³¼ ì €ì¥ ì¤‘...")

# ê²°ê³¼ DataFrame ìƒì„±
result_df = pd.DataFrame({
    'ID': test_df['ID'],
    'generated': predictions
})

# ê²°ê³¼ íŒŒì¼ ì €ì¥
output_filename = './submission_koelectra_base_v3.csv'
result_df.to_csv(output_filename, index=False, encoding='utf-8-sig')

print(f"âœ… {output_filename} ì €ì¥ ì™„ë£Œ")

# =====================================
# 7. ê²°ê³¼ ìš”ì•½
# =====================================
print(f"\nğŸ¯ ì˜ˆì¸¡ ì™„ë£Œ!")
print(f"\nğŸ“‹ ìƒì„±ëœ íŒŒì¼:")
print(f"- {output_filename}")

print(f"\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ í†µê³„:")
print(f"   - í‰ê· : {predictions.mean():.4f}")
print(f"   - í‘œì¤€í¸ì°¨: {predictions.std():.4f}")
print(f"   - ë²”ìœ„: {predictions.min():.4f} ~ {predictions.max():.4f}")
print(f"   - 0.0~0.3: {(predictions < 0.3).sum()}ê°œ")
print(f"   - 0.3~0.7: {((predictions >= 0.3) & (predictions < 0.7)).sum()}ê°œ")
print(f"   - 0.7~1.0: {(predictions >= 0.7).sum()}ê°œ")

print(f"\nğŸ” ëª¨ë¸ ì •ë³´:")
print(f"   - ëª¨ë¸: {model_info['name']}")
print(f"   - ê²½ë¡œ: {model_info['path']}")
print(f"   - ë””ë°”ì´ìŠ¤: {device}")

print(f"\nğŸ’¾ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ: {output_filename}")
print(f"   - ì´ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
print(f"   - íŒŒì¼ í˜•ì‹: CSV (ID, generated)")

print(f"\nğŸ‰ ì¶”ë¡  ì‘ì—… ì™„ë£Œ!")
